/*
Copyright by Lennart Koehler

Research Group Applied Systems Biology - Head: Prof. Dr. Marc Thilo Figge
https://www.leibniz-hki.de/en/applied-systems-biology.html
HKI-Center for Systems Biology of Infection
Leibniz Institute for Natural Product Research and Infection Biology - Hans Knöll Institute (HKI)
Adolf-Reichwein-Straße 23, 07745 Jena, Germany

The project code is licensed under the MIT license.
See the LICENSE file provided with the code for the full license.
*/

#include "CUDABackendManager.h"
#include <iostream>
#include <stdexcept>
#include "CUDABackend.h"
#include "dolphinbackend/Exceptions.h"
CUDABackendManager& CUDABackendManager::getInstance() {
    static CUDABackendManager instance;
    return instance;
}



void CUDABackendManager::initializeGlobalCUDA() {
    // static bool globalInitialized = false;
    // static std::mutex initMutex;
    // static std::condition_variable initCondition;
    // static bool initInProgress = false;
    
    // std::unique_lock<std::mutex> lock(initMutex);
    // cudaStream_t stream1;
    // cudaStreamCreate(&stream1);

} 

std::shared_ptr<CUDABackend> CUDABackendManager::getNewBackendDifferentDevice(const CUDADevice& device){
    CUDADevice newdevice = devices[usedDeviceCounter];
    usedDeviceCounter = ++usedDeviceCounter % nDevices; // keep looping

    return createNewBackend(newdevice);    
}


std::shared_ptr<CUDABackend> CUDABackendManager::getNewBackendSameDevice(const CUDADevice& device){
    return createNewBackend(device);
}

// TODO make stream management more native, different streams in one cuda backend should not be permitted, one origin of truth
std::shared_ptr<CUDABackend> CUDABackendManager::createNewBackend(CUDADevice device) {
    // initializeGlobalCUDA();
    
    if (nDevices == 0) {
        throw dolphin::backend::BackendException(
            "No CUDA devices available", "CUDA", "createNewBackend");
    }
    
    if (devices.empty()) {
        throw dolphin::backend::BackendException(
            "No CUDA devices configured", "CUDA", "createNewBackend");
    }
    
    cudaError_t err = cudaSetDevice(device.id);
    CUDA_CHECK(err, "createNewBackend - cudaSetDevice");
    
    cudaStream_t stream;
    err = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);
    CUDA_CHECK(err, "createNewBackend - cudaStreamCreateWithFlags");
    
    try {
        CUDABackend* backend = CUDABackend::create();
        if (!backend) {
            throw dolphin::backend::BackendException(
                "Failed to create CUDABackend", "CUDA", "createNewBackend");
        }
        
        backend->setStream(stream);
        backend->setDevice(device);
        


        // cudaDeviceSynchronize();
        CUDA_CHECK(err, "createNewBackend - cudaSetDevice reset");

        std::shared_ptr<CUDABackend> cudabackend = std::shared_ptr<CUDABackend>(backend);
        backends.push_back(cudabackend);
        return cudabackend;
    } catch (...) {
        // Clean up stream if backend creation fails
        cudaStreamDestroy(stream);
        throw;
    }
}




size_t CUDABackendManager::getMaxThreads() const {
    std::unique_lock<std::mutex> lock(managerMutex_);
    return maxThreads_;
}

size_t CUDABackendManager::getActiveThreads() const {
    std::unique_lock<std::mutex> lock(managerMutex_);
    return threadBackends_.size();
}

size_t CUDABackendManager::getTotalBackends() const {
    std::unique_lock<std::mutex> lock(managerMutex_);
    return totalCreated_;
}

void CUDABackendManager::cleanup() {
    std::unique_lock<std::mutex> lock(managerMutex_);
    
    // Clean up all active thread backends
    for (auto& pair : threadBackends_) {
        if (pair.second.backend) {
            pair.second.backend->mutableDeconvManager().cleanup();
        }
    }
    threadBackends_.clear();
    

    
    std::cout << "[INFO] Cleaned up CUDA backend manager" << std::endl;
}

cudaStream_t CUDABackendManager::createStream() {
    cudaStream_t stream;
    cudaError_t err = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);
    CUDA_CHECK(err, "createStream - cudaStreamCreateWithFlags");
    return stream;
}














// unused
std::shared_ptr<CUDABackend> CUDABackendManager::getBackendForCurrentThread() {
    std::unique_lock<std::mutex> lock(managerMutex_);
    
    std::thread::id currentThreadId = std::this_thread::get_id();

    // Check if we already have a backend for this thread
    auto it = threadBackends_.find(currentThreadId);
    if (it != threadBackends_.end() && !it->second.inUse) {
        it->second.inUse = true;
        return it->second.backend;
    }
    
    // Check if we can create a new backend
    if (threadBackends_.size() < maxThreads_) {
        CUDADevice device = devices[usedDeviceCounter];
        usedDeviceCounter = ++usedDeviceCounter % nDevices; // keep looping

        auto backend = createNewBackend(device);
        threadBackends_[currentThreadId].backend = std::move(backend);
        totalCreated_++;

        threadBackends_[currentThreadId].inUse = true;
        return threadBackends_[currentThreadId].backend;
    }

    
    // Try again after waiting
    return getBackendForCurrentThread();
}
std::shared_ptr<CUDABackend> CUDABackendManager::getBackendForCurrentThreadSameDevice(CUDADevice device) {
    std::unique_lock<std::mutex> lock(managerMutex_);
    
    std::thread::id currentThreadId = std::this_thread::get_id();

    // Check if we already have a backend for this thread
    auto it = threadBackends_.find(currentThreadId);
    if (it != threadBackends_.end() && !it->second.inUse) {
        it->second.inUse = true;
        return it->second.backend;
    }
    
    // Check if we can create a new backend
    if (threadBackends_.size() < maxThreads_) {

        auto backend = createNewBackend(device);
        threadBackends_[currentThreadId].backend = std::move(backend);
        totalCreated_++;

        threadBackends_[currentThreadId].inUse = true;
        return threadBackends_[currentThreadId].backend;
    }

    
    // Try again after waiting
    return getBackendForCurrentThread();
}


void CUDABackendManager::releaseBackendForCurrentThread(CUDABackend* backend) {
    std::unique_lock<std::mutex> lock(managerMutex_);
    
    std::thread::id currentThreadId = std::this_thread::get_id();
    
    // Check if this thread has an active backend
    auto it = threadBackends_.find(currentThreadId);
    if (it != threadBackends_.end() && it->second.inUse) {
        // Move the backend to the available pool
        it->second.inUse = false;
        
    } else {
        std::cerr << "[WARNING] Attempted to release backend for thread " << currentThreadId
                  << " that doesn't have an active backend" << std::endl;
    }
}

void CUDABackendManager::setMaxThreads(size_t maxThreads) {
    std::unique_lock<std::mutex> lock(managerMutex_);
    maxThreads_ = maxThreads;
}